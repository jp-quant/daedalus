# Daedalus Configuration
# Copy this file to config.yaml and fill in your credentials

# =============================================================================
# STORAGE BACKEND CONFIGURATION
# =============================================================================
# Explicit storage configuration per layer
# This gives full control over where each layer reads/writes data

storage:
  # Ingestion layer storage (where raw segments are written)
  ingestion_storage:
    backend: "local"
    base_dir: "./data"
  
  # ETL input storage (where ETL reads raw segments from)
  etl_storage_input:
    backend: "local"
    base_dir: "./data"
  
  # ETL output storage (where processed Parquet files are written)
  etl_storage_output:
    backend: "local"
    base_dir: "./data"
    # backend: "s3"
    # base_dir: "market-data-vault"
    # s3:
    #   bucket: "market-data-vault"
    #   region: "us-east-1"
    #   aws_access_key_id: null
    #   aws_secret_access_key: null
  
  # ==========================================================================
  # SYNC JOB CONFIGURATION
  # ==========================================================================
  # Sync job runs separately from ETL to upload processed data to S3
  # This allows ETL to run fast on local storage, then sync to durable S3
  #
  # IMPORTANT: This is separate from ETL storage because:
  # - ETL can run fully local for performance
  # - Sync handles local→S3 uploads on a schedule
  # - You can sync even when ETL writes to local
  sync:
    enabled: false  # Set to true when ready to sync to S3
    
    # Source storage (where to read from - typically local processed data)
    source:
      backend: "local"
      base_dir: "./data"
    
    # Destination storage (where to upload to - typically S3)
    destination:
      backend: "s3"
      base_dir: "market-data-vault"
      s3:
        bucket: "market-data-vault"
        region: "us-east-1"
        aws_access_key_id: null
        aws_secret_access_key: null
        max_pool_connections: 50  # boto3 connection pool (increase if max_workers > 10)
    
    # Sync options
    compact_before_upload: false     # Compact small parquet files before upload
    delete_after_transfer: true     # Delete local files after successful upload  
    target_file_size_mb: 100        # Target file size for compaction
    max_workers: 42                  # Parallel upload threads
    interval_seconds: 300           # Seconds between syncs in continuous mode
    
    # Specific paths to sync (empty = sync all processed data)
    # paths: []
    paths:
      - "raw/ready/ccxt/ticker/"
      - "raw/ready/ccxt/orderbook/"
      - "raw/ready/ccxt/trades/"


  # ============================================================================
  # FULL LOCAL EXAMPLES (modify the configs above to switch modes)
  # ============================================================================
  # Ingestion layer storage (where raw segments are written)
  # ingestion_storage:
  #   backend: "local"
  #   base_dir: "F:/"
  
  # ETL input storage (where ETL reads raw segments from)
  # etl_storage_input:
  #   backend: "local"
  #   base_dir: "F:/"
  
  # ETL output storage (where processed Parquet files are written)
  # etl_storage_output:
  #   backend: "local"
  #   base_dir: "F:/"

  # ============================================================================
  # HYBRID MODE EXAMPLES (modify the configs above to switch modes)
  # ============================================================================
  
  # Example 1: Ingest to local, ETL to S3 (RECOMMENDED for production)
  # Combines fast local ingestion with durable S3 storage
  # 
  # ingestion_storage:
  #   backend: "local"
  #   base_dir: "F:/"
  # 
  # etl_storage_input:
  #   backend: "local"
  #   base_dir: "F:/"
  # 
  # etl_storage_output:
  #   backend: "s3"
  #   base_dir: "market-data-vault"
  #   s3:
  #     bucket: "market-data-vault"
  #     region: null
  #     aws_access_key_id: null
  #     aws_secret_access_key: null
  #     max_pool_connections: 50  # Important for ThreadPoolExecutor usage
  
  # Example 2: All S3 (fully cloud-native)
  # 
  # ingestion_storage:
  #   backend: "s3"
  #   base_dir: "market-data-vault"
  #   s3:
  #     bucket: "market-data-vault"
  #     region: null
  # 
  # etl_storage_input:
  #   backend: "s3"
  #   base_dir: "market-data-vault"
  #   s3:
  #     bucket: "market-data-vault"
  #     region: null
  # 
  # etl_storage_output:
  #   backend: "s3"
  #   base_dir: "market-data-vault"
  #   s3:
  #     bucket: "market-data-vault"
  #     region: null
  
  # Example 3: Multi-bucket strategy (different buckets for raw vs processed)
  # 
  # ingestion_storage:
  #   backend: "s3"
  #   base_dir: "raw-data-bucket"
  #   s3:
  #     bucket: "raw-data-bucket"
  # 
  # etl_storage_input:
  #   backend: "s3"
  #   base_dir: "raw-data-bucket"
  #   s3:
  #     bucket: "raw-data-bucket"
  # 
  # etl_storage_output:
  #   backend: "s3"
  #   base_dir: "processed-data-bucket"
  #   s3:
  #     bucket: "processed-data-bucket"
  #     region: "us-west-2"
  
  # Path structure - customize your directory layout
  # All paths are relative to base_dir (applies to both local and S3)
  paths:
    raw_dir: "raw"              # Base for raw ingestion data
    active_subdir: "active"      # Actively writing segments: {base_dir}/{raw_dir}/{active_subdir}/{source}/
    ready_subdir: "ready"        # Ready for ETL: {base_dir}/{raw_dir}/{ready_subdir}/{source}/
    processing_subdir: "processing"  # ETL in-progress
    processed_dir: "processed"   # Processed data: {base_dir}/{processed_dir}/{source}/
    
    # Medallion architecture tier names (bronze → silver → gold)
    # Customize these to match your naming convention
    tier_raw: "bronze"           # Raw data tier (landing zone)
    tier_features: "silver"      # Feature-extracted data tier
    tier_aggregates: "gold"      # Aggregated/final data tier
    
    # State management directory (relative to base_dir)
    state_dir: "temp/state"      # ETL state persistence

# =============================================================================
# DATA SOURCES
# =============================================================================

# Coinbase Advanced Trade API
coinbase:
  api_key: ""
  api_secret: ""
  product_ids:
    - "BTC-USD"
    - "ETH-USD"
    - "SOL-USD"
    - "XRP-USD"
    - "ZEC-USD"
    - "TNSR-USD"
    - "DOGE-USD"
    - "LINK-USD"
    - "ADA-USD"
  channels:
    - "ticker"
    - "level2"
    - "market_trades"
  ws_url: "wss://advanced-trade-ws.coinbase.com"
  level2_batch_size: 10  # Max products per level2 subscription (Coinbase limit)

# =============================================================================
# CCXT (Multi-Exchange Support)
# =============================================================================
ccxt:
  exchanges:
    binanceus:
      max_orderbook_depth: 50
      channels:
        watchTicker: ['BTC/USD', 'ETH/USD', 'SOL/USD', 'XRP/USD', 'BNB/USD', 'ADA/USD', 'DOGE/USD', 'PEPE/USD', 'HYPE/USD', 'LINK/USD', 'FET/USD', 'ONE/USD', 'DOT/USD', 'ICP/USD', 'TRUMP/USD', 'AVAX/USD', 'HBAR/USD', 'KDA/USD', 'ENS/USD', 'VET/USD', 'AAVE/USD', 'JUP/USD', 'SUI/USD', 'THETA/USD', 'FIL/USD', 'APT/USD', 'LTC/USD', 'MANA/USD', 'POL/USD', 'GALA/USD', 'RENDER/USD', 'BCH/USD', 'UNI/USD', 'LPT/USD', 'GRT/USD', 'IOTA/USD', 'XLM/USD', 'NEAR/USD', 'ATOM/USD', 'SHIB/USD', 'CRV/USD', 'RVN/USD', 'VTHO/USD', 'ZIL/USD', 'DGB/USD', 'ALGO/USD', 'ETC/USD', 'BONK/USD', 'SUSHI/USD', 'ME/USD']
        watchTrades: ['BTC/USD', 'ETH/USD', 'SOL/USD', 'XRP/USD', 'BNB/USD', 'ADA/USD', 'DOGE/USD', 'PEPE/USD', 'HYPE/USD', 'LINK/USD', 'FET/USD', 'ONE/USD', 'DOT/USD', 'ICP/USD', 'TRUMP/USD', 'AVAX/USD', 'HBAR/USD', 'KDA/USD', 'ENS/USD', 'VET/USD', 'AAVE/USD', 'JUP/USD', 'SUI/USD', 'THETA/USD', 'FIL/USD', 'APT/USD', 'LTC/USD', 'MANA/USD', 'POL/USD', 'GALA/USD', 'RENDER/USD', 'BCH/USD', 'UNI/USD', 'LPT/USD', 'GRT/USD', 'IOTA/USD', 'XLM/USD', 'NEAR/USD', 'ATOM/USD', 'SHIB/USD', 'CRV/USD', 'RVN/USD', 'VTHO/USD', 'ZIL/USD', 'DGB/USD', 'ALGO/USD', 'ETC/USD', 'BONK/USD', 'SUSHI/USD', 'ME/USD']
        watchOrderBook: ['BTC/USD', 'ETH/USD', 'SOL/USD', 'XRP/USD', 'BNB/USD', 'ADA/USD', 'DOGE/USD', 'PEPE/USD', 'HYPE/USD', 'LINK/USD', 'FET/USD', 'ONE/USD', 'DOT/USD', 'ICP/USD', 'TRUMP/USD', 'AVAX/USD', 'HBAR/USD', 'KDA/USD', 'ENS/USD', 'VET/USD', 'AAVE/USD', 'JUP/USD', 'SUI/USD', 'THETA/USD', 'FIL/USD', 'APT/USD', 'LTC/USD', 'MANA/USD', 'POL/USD', 'GALA/USD', 'RENDER/USD', 'BCH/USD', 'UNI/USD', 'LPT/USD', 'GRT/USD', 'IOTA/USD', 'XLM/USD', 'NEAR/USD', 'ATOM/USD', 'SHIB/USD', 'CRV/USD', 'RVN/USD', 'VTHO/USD', 'ZIL/USD', 'DGB/USD', 'ALGO/USD', 'ETC/USD', 'BONK/USD', 'SUSHI/USD', 'ME/USD']
    coinbaseadvanced:
      max_orderbook_depth: 50
      api_key: ""
      api_secret: ""
      channels:
        watchTicker: ['BTC/USDC', 'ETH/USDC', 'XRP/USDC', 'SOL/USDC', 'ZEC/USDC', 'LINK/USDC', 'DOGE/USDC', 'ADA/USDC', 'SUI/USDC', 'FARTCOIN/USDC', 'PENGU/USDC', 'AAVE/USDC', 'HBAR/USDC', 'MON/USDC', 'AVAX/USDC', 'TAO/USDC', 'BCH/USDC', 'LTC/USDC', 'XLM/USDC', 'PEPE/USDC', 'ICP/USDC', 'BONK/USDC', 'UNI/USDC', 'ONDO/USDC', 'ZEN/USDC']
        watchTrades: ['BTC/USDC', 'ETH/USDC', 'XRP/USDC', 'SOL/USDC', 'ZEC/USDC', 'LINK/USDC', 'DOGE/USDC', 'ADA/USDC', 'SUI/USDC', 'FARTCOIN/USDC', 'PENGU/USDC', 'AAVE/USDC', 'HBAR/USDC', 'MON/USDC', 'AVAX/USDC', 'TAO/USDC', 'BCH/USDC', 'LTC/USDC', 'XLM/USDC', 'PEPE/USDC', 'ICP/USDC', 'BONK/USDC', 'UNI/USDC', 'ONDO/USDC', 'ZEN/USDC']
        watchOrderBook: ['BTC/USDC', 'ETH/USDC', 'XRP/USDC', 'SOL/USDC', 'ZEC/USDC', 'LINK/USDC', 'DOGE/USDC', 'ADA/USDC', 'SUI/USDC', 'FARTCOIN/USDC', 'PENGU/USDC', 'AAVE/USDC', 'HBAR/USDC', 'MON/USDC', 'AVAX/USDC', 'TAO/USDC', 'BCH/USDC', 'LTC/USDC', 'XLM/USDC', 'PEPE/USDC', 'ICP/USDC', 'BONK/USDC', 'UNI/USDC', 'ONDO/USDC', 'ZEN/USDC']

# =============================================================================
# INGESTION LAYER SETTINGS
# =============================================================================
# Controls how data is collected from WebSocket APIs
# Paths determined by storage.paths configuration above
ingestion:
  batch_size: 100                  # Records per batch write
  flush_interval_seconds: 5.0      # Force flush every N seconds
  queue_maxsize: 10000             # Max records in memory queue
  enable_fsync: true               # Ensure data persisted to disk/S3
  auto_reconnect: true             # Reconnect on disconnection
  max_reconnect_attempts: 100000   # Max reconnect attempts
  reconnect_delay: 5.0             # Seconds between reconnect attempts
  segment_max_mb: 100              # Rotate segment after N MB
  
  # Parquet raw format options (when raw_format: "parquet")
  raw_format: "parquet"                       # "ndjson" or "parquet"
  parquet_compression: "zstd"                 # Compression for raw parquet (snappy, gzip, zstd)
  parquet_compression_level: 1                # Compression level (1=fast, 9=max)
  
  # ===========================================================================
  # PARTITIONING CONFIGURATION
  # ===========================================================================
  # Raw data is partitioned using Hive-style directory structure for efficient 
  # downstream ETL processing. This allows ETL to read only relevant partitions
  # instead of scanning all data.
  #
  # Example structure with full partitioning:
  #   raw/ready/ccxt/orderbook/exchange=binanceus/symbol=BTC~USDT/year=2025/month=6/day=19/
  #
  # partition_by options: ["exchange", "symbol"] (default)
  #   - exchange: Partition by exchange name (binanceus, coinbase, etc.)
  #   - symbol: Partition by trading pair (BTC~USDT, ETH~USDT, etc.)
  #
  # enable_date_partition: true (default)
  #   - Adds year/month/day partitions for time-based filtering
  
  partition_by:
    - "exchange"
    - "symbol"
  enable_date_partition: false

# =============================================================================
# ETL LAYER SETTINGS
# =============================================================================
etl:
  compression: "zstd"            # Parquet compression (snappy, gzip, zstd)
  schedule_cron: null              # Optional: "0 * * * *" for hourly ETL
  delete_after_processing: true    # Delete raw segments after successful ETL
  
  # State management for checkpoint/resume and graceful shutdown
  state:
    enabled: true                         # Enable state persistence
    checkpoint_interval_seconds: 300      # Auto-save state every N seconds
    max_state_files: 5                    # Keep N recent state backups
    auto_save_on_shutdown: true           # Save state on SIGINT/SIGTERM
  
  # Channel-specific ETL configuration
  channels:
    ticker:
      partition_cols:
        - "exchange"
        - "symbol"
        - "date"
      processor_options:
        add_derived_fields: true  # Add timestamp partitions, mid-price, spread, ranges

    orderbook:
      partition_cols:
        - "exchange"
        - "symbol"
        - "date"
      processor_options:
        # Structural + rolling feature extraction
        compute_features: true
        max_levels: 20
        bands_bps: [5, 10, 25, 50, 100]

        # Rolling horizons (seconds)
        horizons: [5, 15, 60, 300, 900]

        # Bar aggregation windows (seconds)
        bar_durations: [60, 300, 900, 3600]

        # Stateful features (sequential) - OFI/MLOFI/Regimes/Kyle/VPIN
        enable_stateful: true
        ofi_levels: 10
        ofi_decay_alpha: 0.5

        use_dynamic_spread_regime: true
        spread_regime_window: 300
        spread_tight_percentile: 0.2
        spread_wide_percentile: 0.8
        tight_spread_threshold: 0.0001

        kyle_lambda_window: 300

        enable_vpin: true
        vpin_bucket_volume: 1.0
        vpin_window_buckets: 50

    trades:
      partition_cols:
        - "exchange"
        - "symbol"
        - "date"
      processor_options: {}

# =============================================================================
# FEATURE COMPUTATION CONFIGURATION (New ETL Framework)
# =============================================================================
# These settings control the etl.core.config.FeatureConfig used by TransformExecutor.
# They are research-optimized defaults based on academic literature.
#
# USE THIS SECTION for all ETL feature configuration. The etl.channels.*.processor_options
# section above is being deprecated in favor of this unified configuration.
#
# References:
#   - Cont et al. (2014): OFI price impact
#   - Kyle & Obizhaeva (2016): Microstructure invariance
#   - López de Prado (2018): Volume clocks
#   - Zhang et al. (2019): DeepLOB (20 levels captures "walls")
#   - Xu et al. (2019): Multi-level OFI with decay
#   - Easley et al. (2012): VPIN for flow toxicity

features:
  # ==========================================================================
  # FEATURE CATEGORIES
  # ==========================================================================
  # Options: structural, dynamic, rolling, bars, advanced
  categories:
    - "structural"   # Static per-snapshot: depth, spread, imbalance, microprice
    - "dynamic"      # State-dependent: OFI, MLOFI, log returns, velocity
    - "rolling"      # Time-windowed: realized vol, rolling OFI, regime stats
    # - "bars"       # Enable for gold-tier bar aggregation
    - "advanced"   # Enable for VPIN, Kyle's Lambda

  # ==========================================================================
  # ORDERBOOK DEPTH PARAMETERS
  # ==========================================================================
  # Zhang et al. (2019): 20 levels captures institutional "walls" and hidden liquidity
  depth_levels: 20
  
  # Xu et al. (2019): Multi-level OFI with 10 levels reduces forecast error 15-70%
  ofi_levels: 10
  
  # Liquidity bands in basis points for depth aggregation
  # Wider bands [5, 10, 25, 50, 100] recommended for crypto volatility
  bands_bps:
    - 5      # Tight spread band
    - 10     # Near-spread
    - 25     # Quarter-percent
    - 50     # Half-percent
    - 100    # 1% from mid

  # ==========================================================================
  # ROLLING WINDOW PARAMETERS
  # ==========================================================================
  # Multi-scale windows for mid-frequency (5s to 4h) trading
  rolling_windows:
    - 5      # 5 second micro-trends
    - 15     # 15 second short-term
    - 60     # 1 minute standard
    - 300    # 5 minute medium-term (captures institutional flow patterns)
    - 900    # 15 minute trend detection

  # ==========================================================================
  # BAR AGGREGATION PARAMETERS
  # ==========================================================================
  # Skip sub-minute bars (redundant with HF @ 1s emission)
  bar_interval_seconds: 60
  bar_durations:
    - 60     # 1 minute
    - 300    # 5 minute
    - 900    # 15 minute
    - 3600   # 1 hour

  # ==========================================================================
  # OFI / MLOFI PARAMETERS (Cont 2014, Xu 2019)
  # ==========================================================================
  # Exponential decay alpha for MLOFI level weighting: w_i = exp(-alpha * i)
  # Higher alpha = more focus on L1, lower = more uniform across levels
  ofi_decay_alpha: 0.5

  # ==========================================================================
  # KYLE'S LAMBDA PARAMETERS (Kyle & Obizhaeva 2016)
  # ==========================================================================
  # Rolling window for price impact estimation
  kyle_lambda_window: 300  # 5 minute window

  # ==========================================================================
  # VPIN PARAMETERS (Easley et al. 2012)
  # ==========================================================================
  # Enable VPIN (Volume-Synchronized Probability of Informed Trading)
  enable_vpin: true
  
  # Volume bucket size (normalize by typical trade size)
  vpin_bucket_size: 1.0
  
  # Rolling window in volume buckets
  vpin_window_buckets: 50

  # ==========================================================================
  # SPREAD REGIME PARAMETERS
  # ==========================================================================
  # Dynamic percentile-based spread regime detection
  use_dynamic_spread_regime: true
  
  # Window for spread percentile calculation
  spread_regime_window: 300  # 5 minute window
  
  # Percentile thresholds for regime classification
  spread_tight_percentile: 0.2   # Bottom 20% = tight spread
  spread_wide_percentile: 0.8    # Top 20% = wide spread
  
  # Fallback static thresholds (used if use_dynamic_spread_regime: false)
  spread_tight_threshold_bps: 5.0    # Below this = tight spread
  spread_wide_threshold_bps: 20.0    # Above this = wide spread

  # ==========================================================================
  # STATEFUL FEATURE TOGGLES
  # ==========================================================================
  # Enable stateful features (OFI, MLOFI, regime tracking, Kyle's Lambda)
  enable_stateful: true

  # ==========================================================================
  # STORAGE OPTIMIZATION
  # ==========================================================================
  # Drop raw bids/asks arrays after structural feature extraction.
  # Default: true - reduces silver tier storage by ~60-80%
  # Set to false if downstream processes need the full orderbook arrays.
  drop_raw_book_arrays: true

# =============================================================================
# LOGGING
# =============================================================================
log_level: "INFO"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
