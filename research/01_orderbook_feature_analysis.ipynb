{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89840e9",
   "metadata": {},
   "source": [
    "# Quantitative Crypto Research - Orderbook Feature Analysis\n",
    "\n",
    "## Research Context\n",
    "\n",
    "**Researcher**: Senior Quant - Crypto Markets  \n",
    "**Goal**: Extract alpha from high-frequency orderbook microstructure  \n",
    "**Asset**: BTC-USD on Coinbase Advanced  \n",
    "**Data**: 197 engineered features from L2 orderbook snapshots + trades\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "1. **Can orderbook microstructure predict short-term price direction?**\n",
    "2. **Which features carry the most predictive signal?**\n",
    "3. **What are the optimal prediction horizons for our feature frequency?**\n",
    "4. **How do different ML models compare on this high-frequency data?**\n",
    "\n",
    "### Key References\n",
    "- Cont et al. (2014): Order Flow Imbalance (OFI) predicts price changes\n",
    "- Kyle (1985): Lambda measures market impact\n",
    "- López de Prado (2018): Financial ML best practices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c80aa",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c08fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, classification_report,\n",
    "    confusion_matrix, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(f\"Research session started: {datetime.now()}\")\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path(\"data/processed/silver/orderbook\")\n",
    "EXCHANGE = \"coinbaseadvanced\"\n",
    "SYMBOL = \"BTC-USD\"\n",
    "\n",
    "# For initial exploration, load a sample (1 day)\n",
    "# For production modeling, use full scan_parquet\n",
    "SAMPLE_MODE = True  # Set False for full data\n",
    "\n",
    "if SAMPLE_MODE:\n",
    "    # Load specific partition for fast iteration\n",
    "    sample_path = DATA_ROOT / f\"exchange={EXCHANGE}/symbol={SYMBOL}/year=2026/month=1/day=26\"\n",
    "    print(f\"Sample mode: Loading from {sample_path}\")\n",
    "    df = pl.read_parquet(f\"{sample_path}/**/*.parquet\")\n",
    "else:\n",
    "    # Full lazy scan for production\n",
    "    full_path = DATA_ROOT / f\"exchange={EXCHANGE}/symbol={SYMBOL}/**/*.parquet\"\n",
    "    print(f\"Full mode: Scanning {full_path}\")\n",
    "    df = pl.scan_parquet(str(full_path)).collect()\n",
    "\n",
    "print(f\"\\nLoaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory: {df.estimated_size('mb'):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE CATEGORIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define feature groups for analysis\n",
    "FEATURE_GROUPS = {\n",
    "    'meta': ['timestamp', 'capture_ts', 'symbol', 'exchange', 'year', 'month', 'day', 'hour'],\n",
    "    'price_levels': [c for c in df.columns if 'price_L' in c],\n",
    "    'size_levels': [c for c in df.columns if 'size_L' in c and 'bid' in c or 'ask' in c],\n",
    "    'core_prices': ['best_bid', 'best_ask', 'mid_price', 'spread', 'relative_spread', 'microprice'],\n",
    "    'imbalances': [c for c in df.columns if 'imbalance' in c.lower()],\n",
    "    'depth': [c for c in df.columns if 'depth' in c.lower() or 'total_' in c],\n",
    "    'ofi': [c for c in df.columns if 'ofi' in c.lower() or 'mlofi' in c.lower()],\n",
    "    'dynamics': [c for c in df.columns if any(x in c for x in ['velocity', 'acceleration', 'log_return'])],\n",
    "    'rolling': [c for c in df.columns if 'rolling' in c.lower() or 'realized_vol' in c or 'mean_' in c],\n",
    "    'advanced': [c for c in df.columns if any(x in c for x in ['kyle', 'vpin', 'toxicity', 'lambda'])],\n",
    "    'liquidity': [c for c in df.columns if any(x in c for x in ['vwap', 'smart', 'concentration', 'slope'])],\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(\"Feature Group Summary:\")\n",
    "print(\"=\" * 50)\n",
    "total_categorized = 0\n",
    "for group, cols in FEATURE_GROUPS.items():\n",
    "    existing = [c for c in cols if c in df.columns]\n",
    "    print(f\"{group:15s}: {len(existing):3d} features\")\n",
    "    total_categorized += len(existing)\n",
    "\n",
    "# Find uncategorized\n",
    "all_categorized = set()\n",
    "for cols in FEATURE_GROUPS.values():\n",
    "    all_categorized.update(cols)\n",
    "uncategorized = [c for c in df.columns if c not in all_categorized]\n",
    "print(f\"{'uncategorized':15s}: {len(uncategorized):3d} features\")\n",
    "print(f\"\\nTotal columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35834544",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEMPORAL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Check timestamp distribution\n",
    "df = df.sort('timestamp')\n",
    "\n",
    "# Calculate inter-arrival times\n",
    "df = df.with_columns([\n",
    "    (pl.col('timestamp').diff().dt.total_milliseconds() / 1000).alias('delta_seconds')\n",
    "])\n",
    "\n",
    "print(\"Timestamp Analysis:\")\n",
    "print(f\"  Start: {df['timestamp'].min()}\")\n",
    "print(f\"  End:   {df['timestamp'].max()}\")\n",
    "print(f\"  Duration: {(df['timestamp'].max() - df['timestamp'].min())}\")\n",
    "\n",
    "print(f\"\\nInter-arrival time (seconds):\")\n",
    "delta_stats = df.select('delta_seconds').drop_nulls().describe()\n",
    "print(delta_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRICE DYNAMICS VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Sample for plotting (every 100th row for large datasets)\n",
    "sample_rate = max(1, len(df) // 5000)\n",
    "df_plot = df.gather_every(sample_rate).to_pandas()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    subplot_titles=('Mid Price', 'Spread (bps)', 'Order Flow Imbalance', 'L1 Imbalance'),\n",
    "    row_heights=[0.3, 0.2, 0.25, 0.25]\n",
    ")\n",
    "\n",
    "# Mid price\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_plot['timestamp'], y=df_plot['mid_price'], \n",
    "               name='Mid Price', line=dict(color='blue', width=1)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Spread in bps\n",
    "if 'relative_spread' in df_plot.columns:\n",
    "    spread_bps = df_plot['relative_spread'] * 10000\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_plot['timestamp'], y=spread_bps,\n",
    "                   name='Spread (bps)', line=dict(color='orange', width=1)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# OFI\n",
    "if 'ofi' in df_plot.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_plot['timestamp'], y=df_plot['ofi'],\n",
    "                   name='OFI', line=dict(color='green', width=1)),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "# L1 Imbalance\n",
    "if 'imbalance_L1' in df_plot.columns:\n",
    "    colors = ['red' if x < 0 else 'green' for x in df_plot['imbalance_L1']]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_plot['timestamp'], y=df_plot['imbalance_L1'],\n",
    "               name='L1 Imbalance', marker_color=colors),\n",
    "        row=4, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=f\"{SYMBOL} Orderbook Microstructure\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE DISTRIBUTIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Select key features for distribution analysis\n",
    "key_features = [\n",
    "    'relative_spread', 'imbalance_L1', 'ofi', 'microprice',\n",
    "    'smart_depth_imbalance', 'total_depth_20'\n",
    "]\n",
    "key_features = [f for f in key_features if f in df.columns]\n",
    "\n",
    "if key_features:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feat in enumerate(key_features[:6]):\n",
    "        data = df[feat].drop_nulls().to_numpy()\n",
    "        \n",
    "        # Clip outliers for visualization\n",
    "        q01, q99 = np.percentile(data, [1, 99])\n",
    "        data_clipped = np.clip(data, q01, q99)\n",
    "        \n",
    "        axes[i].hist(data_clipped, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'{feat}\\nmean={np.mean(data):.4f}, std={np.std(data):.4f}')\n",
    "        axes[i].axvline(np.mean(data), color='red', linestyle='--', label='mean')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Key Feature Distributions', y=1.02, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890b335",
   "metadata": {},
   "source": [
    "## 3. Target Engineering\n",
    "\n",
    "We create multiple prediction targets at different horizons to understand which timeframes our features can predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acee4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TARGET VARIABLE CREATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define prediction horizons (in number of ticks/rows)\n",
    "# Adjust based on your data frequency\n",
    "HORIZONS = {\n",
    "    '1tick': 1,\n",
    "    '5tick': 5,\n",
    "    '10tick': 10,\n",
    "    '30tick': 30,\n",
    "    '60tick': 60,\n",
    "}\n",
    "\n",
    "# Create forward returns and direction targets\n",
    "target_exprs = []\n",
    "for name, shift in HORIZONS.items():\n",
    "    # Forward log return\n",
    "    target_exprs.append(\n",
    "        (pl.col('mid_price').shift(-shift) / pl.col('mid_price')).log().alias(f'ret_{name}')\n",
    "    )\n",
    "    # Binary direction (1 = up, 0 = down)\n",
    "    target_exprs.append(\n",
    "        (pl.col('mid_price').shift(-shift) > pl.col('mid_price')).cast(pl.Int8).alias(f'dir_{name}')\n",
    "    )\n",
    "\n",
    "df = df.with_columns(target_exprs)\n",
    "\n",
    "# Verify target creation\n",
    "print(\"Target Variables Created:\")\n",
    "print(\"=\" * 50)\n",
    "for name in HORIZONS.keys():\n",
    "    ret_col = f'ret_{name}'\n",
    "    dir_col = f'dir_{name}'\n",
    "    \n",
    "    ret_stats = df.select(ret_col).drop_nulls()\n",
    "    dir_stats = df.select(dir_col).drop_nulls()\n",
    "    \n",
    "    up_pct = dir_stats[dir_col].sum() / len(dir_stats) * 100\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Return: mean={ret_stats[ret_col].mean():.6f}, std={ret_stats[ret_col].std():.6f}\")\n",
    "    print(f\"  Direction: {up_pct:.1f}% up, {100-up_pct:.1f}% down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b282ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TARGET AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "# Check if returns are autocorrelated (mean-reverting or trending)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Return Autocorrelation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name in HORIZONS.keys():\n",
    "    ret_col = f'ret_{name}'\n",
    "    returns = df[ret_col].drop_nulls().to_numpy()\n",
    "    \n",
    "    # Lag-1 autocorrelation\n",
    "    if len(returns) > 100:\n",
    "        autocorr_1 = np.corrcoef(returns[:-1], returns[1:])[0, 1]\n",
    "        autocorr_5 = np.corrcoef(returns[:-5], returns[5:])[0, 1]\n",
    "        \n",
    "        print(f\"\\n{ret_col}:\")\n",
    "        print(f\"  Lag-1 autocorr: {autocorr_1:.4f}\")\n",
    "        print(f\"  Lag-5 autocorr: {autocorr_5:.4f}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if autocorr_1 < -0.05:\n",
    "            print(f\"  → Mean-reverting (negative autocorr)\")\n",
    "        elif autocorr_1 > 0.05:\n",
    "            print(f\"  → Trending (positive autocorr)\")\n",
    "        else:\n",
    "            print(f\"  → Random walk (near-zero autocorr)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8db27e",
   "metadata": {},
   "source": [
    "## 4. Feature-Target Correlation Analysis\n",
    "\n",
    "Identify which features have predictive power for our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE-TARGET CORRELATIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Select numeric features (exclude meta, targets)\n",
    "meta_cols = FEATURE_GROUPS['meta']\n",
    "target_cols = [c for c in df.columns if c.startswith('ret_') or c.startswith('dir_')]\n",
    "exclude_cols = meta_cols + target_cols + ['delta_seconds']\n",
    "\n",
    "feature_cols = [\n",
    "    c for c in df.columns \n",
    "    if c not in exclude_cols \n",
    "    and df[c].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]\n",
    "]\n",
    "\n",
    "print(f\"Analyzing {len(feature_cols)} numeric features against targets...\")\n",
    "\n",
    "# Convert to pandas for correlation analysis\n",
    "df_corr = df.select(feature_cols + ['ret_5tick', 'dir_5tick']).drop_nulls().to_pandas()\n",
    "\n",
    "# Calculate correlations with 5-tick return (our primary target)\n",
    "correlations = {}\n",
    "for col in feature_cols:\n",
    "    if col in df_corr.columns:\n",
    "        corr = df_corr[col].corr(df_corr['ret_5tick'])\n",
    "        correlations[col] = corr\n",
    "\n",
    "# Sort by absolute correlation\n",
    "corr_sorted = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nTop 20 Features by |Correlation| with ret_5tick:\")\n",
    "print(\"=\" * 60)\n",
    "for feat, corr in corr_sorted[:20]:\n",
    "    bar = '█' * int(abs(corr) * 50)\n",
    "    sign = '+' if corr > 0 else '-'\n",
    "    print(f\"{feat:40s} {sign}{abs(corr):.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333edcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORRELATION HEATMAP FOR TOP FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "# Select top correlated features\n",
    "top_features = [f for f, _ in corr_sorted[:15]]\n",
    "top_features_with_target = top_features + ['ret_5tick']\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = df_corr[top_features_with_target].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True, \n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    ax=ax\n",
    ")\n",
    "plt.title('Feature Correlation Matrix (Top 15 + Target)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc4311",
   "metadata": {},
   "source": [
    "## 5. ML Model Training with AutoGluon\n",
    "\n",
    "We use AutoGluon's TabularPredictor for automated model selection and ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AUTOGLUON SETUP\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    AUTOGLUON_AVAILABLE = True\n",
    "    print(\"AutoGluon available ✓\")\n",
    "except ImportError:\n",
    "    AUTOGLUON_AVAILABLE = False\n",
    "    print(\"AutoGluon not installed. Install with: pip install autogluon\")\n",
    "    print(\"Falling back to sklearn models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff92edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPARATION FOR MODELING\n",
    "# ============================================================================\n",
    "\n",
    "# Use top correlated features + some domain knowledge features\n",
    "model_features = list(set(\n",
    "    top_features + \n",
    "    ['ofi', 'imbalance_L1', 'relative_spread', 'mid_price', 'microprice']\n",
    "))\n",
    "model_features = [f for f in model_features if f in df.columns]\n",
    "\n",
    "# Target\n",
    "TARGET = 'dir_5tick'  # Binary classification\n",
    "\n",
    "# Prepare dataset\n",
    "df_model = df.select(model_features + [TARGET, 'timestamp']).drop_nulls()\n",
    "\n",
    "# Time-based train/test split (80/20)\n",
    "# IMPORTANT: For time series, we split by time, not random\n",
    "n_total = len(df_model)\n",
    "n_train = int(n_total * 0.8)\n",
    "\n",
    "train_df = df_model[:n_train].to_pandas()\n",
    "test_df = df_model[n_train:].to_pandas()\n",
    "\n",
    "# Remove timestamp for modeling\n",
    "train_df = train_df.drop(columns=['timestamp'])\n",
    "test_df = test_df.drop(columns=['timestamp'])\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"Features: {len(model_features)}\")\n",
    "print(f\"Target: {TARGET}\")\n",
    "print(f\"\\nClass balance (train): {train_df[TARGET].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae716d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN AUTOGLUON MODEL\n",
    "# ============================================================================\n",
    "\n",
    "if AUTOGLUON_AVAILABLE:\n",
    "    # Model save path\n",
    "    MODEL_PATH = f\"models/autogluon_{SYMBOL.replace('-', '_')}_{TARGET}\"\n",
    "    \n",
    "    # Create predictor\n",
    "    predictor = TabularPredictor(\n",
    "        label=TARGET,\n",
    "        path=MODEL_PATH,\n",
    "        problem_type='binary',\n",
    "        eval_metric='roc_auc',  # Optimize for AUC\n",
    "    )\n",
    "    \n",
    "    # Train with time limit (adjust based on your hardware)\n",
    "    # presets: 'best_quality', 'high_quality', 'good_quality', 'medium_quality'\n",
    "    predictor.fit(\n",
    "        train_data=train_df,\n",
    "        presets='medium_quality',  # Start fast, increase for production\n",
    "        time_limit=300,  # 5 minutes max\n",
    "        verbosity=2,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "else:\n",
    "    print(\"Skipping AutoGluon training (not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "if AUTOGLUON_AVAILABLE and 'predictor' in dir():\n",
    "    # Leaderboard\n",
    "    print(\"Model Leaderboard:\")\n",
    "    print(\"=\" * 80)\n",
    "    leaderboard = predictor.leaderboard(test_df, silent=True)\n",
    "    print(leaderboard.to_string())\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = predictor.predict(test_df)\n",
    "    y_pred_proba = predictor.predict_proba(test_df)\n",
    "    y_true = test_df[TARGET]\n",
    "    \n",
    "    # Metrics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Test Set Performance:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC:  {roc_auc_score(y_true, y_pred_proba[1]):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Down', 'Up']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "if AUTOGLUON_AVAILABLE and 'predictor' in dir():\n",
    "    print(\"Computing feature importance (permutation-based)...\")\n",
    "    \n",
    "    importance = predictor.feature_importance(test_df)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    importance_sorted = importance.sort_values('importance', ascending=True)\n",
    "    \n",
    "    ax.barh(importance_sorted.index, importance_sorted['importance'])\n",
    "    ax.set_xlabel('Permutation Importance')\n",
    "    ax.set_title(f'Feature Importance for {TARGET} Prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be22588",
   "metadata": {},
   "source": [
    "## 6. Sklearn Fallback Models\n",
    "\n",
    "If AutoGluon is not available, we use standard sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754809c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SKLEARN MODELS (FALLBACK)\n",
    "# ============================================================================\n",
    "\n",
    "if not AUTOGLUON_AVAILABLE:\n",
    "    from sklearn.ensemble import (\n",
    "        RandomForestClassifier, \n",
    "        GradientBoostingClassifier,\n",
    "        HistGradientBoostingClassifier\n",
    "    )\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_df.drop(columns=[TARGET])\n",
    "    y_train = train_df[TARGET]\n",
    "    X_test = test_df.drop(columns=[TARGET])\n",
    "    y_test = test_df[TARGET]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Models to try\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "        'Hist Gradient Boosting': HistGradientBoostingClassifier(max_iter=100),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Use scaled data for linear models\n",
    "        if 'Logistic' in name:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        results.append({'Model': name, 'Accuracy': acc, 'AUC': auc})\n",
    "        print(f\"  Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Results summary\n",
    "    results_df = pd.DataFrame(results).sort_values('AUC', ascending=False)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Model Comparison:\")\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fac4d5",
   "metadata": {},
   "source": [
    "## 7. Strategy Backtest Preview\n",
    "\n",
    "Quick simulation of a trading strategy based on model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe48377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SIMPLE STRATEGY BACKTEST\n",
    "# ============================================================================\n",
    "\n",
    "# Get predictions (use AutoGluon if available, else best sklearn model)\n",
    "if AUTOGLUON_AVAILABLE and 'predictor' in dir():\n",
    "    predictions_proba = predictor.predict_proba(test_df)[1].values\n",
    "else:\n",
    "    # Use the best sklearn model\n",
    "    best_model = models['Hist Gradient Boosting']\n",
    "    predictions_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Prepare backtest data\n",
    "backtest_df = test_df.copy()\n",
    "backtest_df['pred_proba'] = predictions_proba\n",
    "backtest_df['signal'] = (backtest_df['pred_proba'] > 0.5).astype(int) * 2 - 1  # -1 or +1\n",
    "\n",
    "# Calculate actual returns for the horizon\n",
    "# Note: This uses 5-tick forward return which may need adjustment\n",
    "backtest_df['actual_return'] = backtest_df[TARGET].map({1: 0.001, 0: -0.001})  # Simplified\n",
    "\n",
    "# Strategy return\n",
    "backtest_df['strategy_return'] = backtest_df['signal'] * backtest_df['actual_return']\n",
    "\n",
    "# Cumulative returns\n",
    "backtest_df['cum_return'] = (1 + backtest_df['strategy_return']).cumprod()\n",
    "backtest_df['cum_benchmark'] = (1 + backtest_df['actual_return']).cumprod()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(backtest_df['cum_return'].values, label='Strategy', linewidth=1.5)\n",
    "ax.plot(backtest_df['cum_benchmark'].values, label='Buy & Hold', linewidth=1.5, alpha=0.7)\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Trade #')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_title('Strategy vs Buy & Hold (Simplified Backtest)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "total_trades = len(backtest_df)\n",
    "winning_trades = (backtest_df['strategy_return'] > 0).sum()\n",
    "total_return = backtest_df['cum_return'].iloc[-1] - 1\n",
    "\n",
    "print(f\"\\nBacktest Summary:\")\n",
    "print(f\"  Total trades: {total_trades:,}\")\n",
    "print(f\"  Win rate: {winning_trades/total_trades*100:.1f}%\")\n",
    "print(f\"  Total return: {total_return*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330dda6",
   "metadata": {},
   "source": [
    "## 8. Next Steps & Research Agenda\n",
    "\n",
    "### Immediate Actions\n",
    "1. **Expand dataset**: Load full historical data (multiple days/weeks)\n",
    "2. **Feature engineering**: Add more lagged features, interaction terms\n",
    "3. **Hyperparameter tuning**: Increase AutoGluon time_limit for better models\n",
    "4. **Walk-forward validation**: Implement proper time-series cross-validation\n",
    "\n",
    "### Research Directions\n",
    "1. **Multi-asset models**: Train on multiple symbols, look for transfer learning\n",
    "2. **Regime detection**: Build volatility regime classifier\n",
    "3. **Ensemble strategies**: Combine predictions from different horizons\n",
    "4. **Cost analysis**: Add realistic transaction costs and slippage\n",
    "\n",
    "### Production Considerations\n",
    "1. Model retraining frequency\n",
    "2. Feature pipeline latency\n",
    "3. Position sizing based on prediction confidence\n",
    "4. Risk management rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESEARCH ARTIFACTS\n",
    "# ============================================================================\n",
    "\n",
    "# Save feature correlation results\n",
    "corr_df = pd.DataFrame(corr_sorted, columns=['feature', 'correlation_with_ret5tick'])\n",
    "corr_df.to_csv('research/feature_correlations.csv', index=False)\n",
    "print(\"Saved: research/feature_correlations.csv\")\n",
    "\n",
    "# Save model features list\n",
    "with open('research/model_features.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(model_features))\n",
    "print(\"Saved: research/model_features.txt\")\n",
    "\n",
    "print(\"\\nResearch session complete!\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
